# Finetune pre-trained T5 3B on WikiDiff

import language.fruit.tasks


include 't5x/configs/runs/finetune.gin'
include 't5x/examples/t5/t5_1_0/3B.gin'


TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 512}
TRAIN_STEPS = 1_030_000
BATCH_SIZE = 128
INITIAL_CHECKPOINT_PATH = ''
USE_CACHED_TASKS = False
partitioning.PjitPartitioner:
  model_parallel_submesh=(4,8,1,2)

# Ensure truncation during inference
infer_eval/utils.DatasetConfig:
  task_feature_lengths = %TASK_FEATURE_LENGTHS

trainer.Trainer:
  num_microbatches = 4

utils.SaveCheckpointConfig:
  keep = 1
